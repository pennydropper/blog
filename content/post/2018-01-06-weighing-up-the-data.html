---
title: Weighing up the data
author: James
date: '2018-01-06'
categories:
  - personal
  - R
tags:
  - loess
  - regression
slug: weighing-up-the-data
---



<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Health and wellbeing is one of my 5 core values, and I extended our new beginning at the start of 2017 to establish new habits to live these values. The new habits included:</p>
<ul>
<li>Cycling to work where possible;</li>
<li>Cutting out “gratuitous” sugar, i.e. confectionary and cakes;</li>
<li>Reducing the evening meal to practically a snack, thus making it my smallest meal of the day.</li>
<li>Following the <a href="http://www.smh.com.au/lifestyle/diet-and-fitness/the-sevenminute-workout-does-it-really-work-20130904-2t66i.html">7 minute workout</a> 3 days per week, though I’ve embellished the exercises and extended it to a 30 minute workout 3 times per week.</li>
</ul>
<p>Weight loss was not my primary objective but I was curious to see how these new habits would affect my weight. My philosophy is well summarised by this article about <a href="http://www.abc.net.au/news/health/2018-01-21/set-aside-losing-weight-focus-on-healthy-behaviours/9345648">focussing on healthy habits</a>.</p>
<p>This blog post uses data to tell the story of the effect of adopting these habits on my weight. As you will see, I lost a considerable amount of weight, which was satisfying, but the study also shows the surprising day-to-day variablility in my weight measure.</p>
<p>To be honest, I’m a little averse to sharing this story in case it comes across as vain or insensitive to friends and others who struggle with their weight. I see myself as very fortunate to have found a framework that has allowed me to achieve this weight loss - adopting these habits was relatively easy for me. I’m really sharing this story as a very personal account of some simple data collection and analysis - I’m treating myself as a “human guinea pig”.</p>
</div>
<div id="measurement-methods" class="section level1">
<h1>Measurement Methods</h1>
<p>Measuing my weight was very simple:</p>
<ul>
<li>I weigh myself each morning as the first thing after I get out of bed (well, actually the second thing) and before I even drink some water. For consistency, I weigh myself wearing my pyjamas, though I realised in winter that the additional t-shirt added 400g to my weight.</li>
<li>We use a Weight Watchers branded bathroom scales with a digital display. As the recorded weight can vary from measure to measure, I tare, weigh myself and repeat until I record the same weight twice in succession. On the majority of mornings, about 19 out of 20, I only need to tare twice.</li>
<li>I simply record the weight with pencil on a sheet taped to the inside of the bathroom cabinet.</li>
<li>The scales show 50g increments though I only recorded the 100g increment.</li>
</ul>
<p>I adopted this routine from 26 March 2017, though I started the daily weigh-in from 13 Feburary and made a feeble attempt to record the readings in the Numbers app on my phone. This recording method was way too complex and unreliable - it shows that there is still room for old technology. For the record, my weight reading on 13 February was 94.7kg.</p>
<p>Some days have no reading, particularly when I’ve been away from home. This represents reality and, as a data scientist, I have left the days blank in some cases or imputed the reading by copying the previous day’s reading.</p>
</div>
<div id="the-raw-data" class="section level1">
<h1>The Raw Data</h1>
<p><img src="/post/2018-01-06-weighing-up-the-data_files/figure-html/chart.raw.data-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>This chart shows that weight loss was consistent and steady from April through to the start of August.</p>
<p>From August to December, the weight loss has tapered off but there still appears to be a gradual decline in weight.</p>
<p>There is a definite dip in weight at the start of September, which coincides with a week when I battled a cold. My weight then picks up in the latter half of September, which coincides with 50th birthday celebrations. His weight does not really fall below the smoothed line again until mid-October. The slight increase in November may coincide with a temporary habit of consuming raw almonds by the handful at work; I cut this habit by the start of December.</p>
<p>My weight has clearly picked up from Christmas; like most people, I overate over the Christmas / New Year period and broke my normal exercise routines.</p>
<p>What also stands out is the variability of the recorded weights above and below the smoothed line (line of best fit). I suspect that this is due to a range of reasons:</p>
<ul>
<li>Variability in the scales.</li>
<li>Day of the week effect as I tended to eat more on weekend days than week days.</li>
<li>Other physiological effects, such as level of hydration and food in my digestive system.</li>
</ul>
<p>As a data scientist, I’m particularly interested in the residuals, i.e. the variation between the line of best fit and the actual reading.</p>
<p>For data scientists, I used the <a href="https://en.wikipedia.org/wiki/Local_regression">Loess</a> approach to generating the line of best fit with span = 0.2. I experimented with a range of spans until I found a line that I judged as a good fit for the points. I considered using an exponential smoothing method but chose Loess on the assumption that the residuals were far greater than the true day-to-day movement, thus the short-term future readings were as valid to include in the smoothing function as the short-term recent readings.</p>
<p>The “line of best fit” should represent my normalised weight once the random affects are removed. It should represent my “true” weight. From a statistics point of view, I had the following train of thought:</p>
<ol style="list-style-type: decimal">
<li>There should be a better way to determine the optimum span.</li>
<li>I played around with optimising the span using k-folds analysis, but the optimum result was the minimum span which produced a very volatile smoothed line that almost tracked through each point. i.e. the smoothed line looked like it was overfitting the data.</li>
<li>I also played around smoothing splines, and it produced a smoothed line that was overfitting even more.</li>
<li>Then I considered the pattern in the residuals, which led to a new theory; there is moderate auto-correlation in the residuals. This auto-correlation hinders methods to find an optimised smoothing lines that don’t “overfit” the data.</li>
</ol>
</div>
<div id="analysis-of-the-variations-from-the-smoothed-line" class="section level1">
<h1>Analysis of the variations from the smoothed line</h1>
<p>By adding the predictions and residuals to the data set, we can understand some of the variations better.</p>
<p><img src="/post/2018-01-06-weighing-up-the-data_files/figure-html/daily.residuals-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>The spread of the points above and below the zero-line looks reasonable.</p>
<p>Statistically speaking, there is a chance that the frequency of the points above and below the line is not random. Later, I’ll discuss some interesting patterns in the sequence of residuals.</p>
<p><strong>Variations from month-to-month</strong> This box plot shows the spread of the residuals within each month. Box plots are regularly used by data scientists to represent the spread of readings. The points and line below the box represent the bottom 25% of the residuals The bottom half of the box represents the next 25% of the residuals (the second quartile), and so on. The points above and below the box represent outliers. I’ve added a point in the middle of the box to represent the average of the residuals, which is usually close to the median but rarely exactly the same.</p>
<p>My key conclusion from this chart is that the residuals are evenly spread around zero, or close to zero, for every month, which helps this analysis.</p>
<p><img src="/post/2018-01-06-weighing-up-the-data_files/figure-html/residuals.by.month-1.png" width="672" /></p>
<p><strong>Variations between days of the week</strong></p>
<p>What if we look at the distribution of residuals based on the day of the week. We will differentiate between the period before 1 August, between 1 August and Christmas, and after Christmas, which is around where we observe “inflection” points.</p>
<p><img src="/post/2018-01-06-weighing-up-the-data_files/figure-html/residuals.by.dow-1.png" width="672" /></p>
<p>The box plots show some interesting features.</p>
<p>For the Pre-August period:</p>
<ul>
<li>The median residual for sunday is around +0.25kg, whereas the median residual for Saturday is around -0.25kg.</li>
<li>This is consistent with different eating patterns and activity levels on weekends compared to weekedays.</li>
</ul>
<p>For the Aug-Xmas period:</p>
<ul>
<li>The highest residual is Monday, with a median of around +0.5kg.</li>
<li>The lowest residual is Thursday with a median of around -0.2kg.</li>
<li>Friday’s and Saturday’s median residual is close to zero.</li>
</ul>
<div id="adjust-for-day-of-the-week" class="section level3">
<h3>Adjust for Day of the Week</h3>
<p>What if we adjust for the day of the week effect? Statistically speaking, this is transforming the data to remove the effect of the day of the week (assuming my hypothesis that the day of the week does affect my weigh-in).</p>
<pre class="r"><code>wts.pred &lt;- wts.pred %&gt;%
  mutate(period = factor(case_when(
    date &lt; dmy(&quot;1/8/2017&quot;) ~ &quot;Pre-Aug&quot;,
    date &lt; dmy(&quot;25/12/2017&quot;) ~ &quot;Aug-Dec&quot;,
    TRUE ~ &quot;Post Xmas&quot;), levels = c(&quot;Pre-Aug&quot;, &quot;Aug-Dec&quot;, &quot;Post Xmas&quot;))
  )

wts.adj &lt;- wts.pred %&gt;%
  group_by(period, wk.day) %&gt;%
  summarise(wk.day.eff = median(resid))

with_plus &lt;- function(x, ...) {
  paste0(if_else(x &gt; 0, &quot;+&quot;, &quot;&quot;), str_trim(format(x, ...)))
}

wts.adj %&gt;%
  spread(key = period, value = wk.day.eff) %&gt;%
  mutate_if(is.numeric, with_plus, digits = 1) %&gt;%
  rename(Day = wk.day) %&gt;%
  knitr::kable(caption = &quot;Day of the week adjustments&quot;,
               align = c(&quot;l&quot;, rep(&quot;r&quot;, 3)))</code></pre>
<table>
<caption>(#tab:dow.adj)Day of the week adjustments</caption>
<thead>
<tr class="header">
<th align="left">Day</th>
<th align="right">Pre-Aug</th>
<th align="right">Aug-Dec</th>
<th align="right">Post Xmas</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Sun</td>
<td align="right">+0.34</td>
<td align="right">+0.09</td>
<td align="right">-0.16</td>
</tr>
<tr class="even">
<td align="left">Mon</td>
<td align="right">+0.21</td>
<td align="right">+0.37</td>
<td align="right">+0.39</td>
</tr>
<tr class="odd">
<td align="left">Tue</td>
<td align="right">+0.05</td>
<td align="right">-0.08</td>
<td align="right">+0.35</td>
</tr>
<tr class="even">
<td align="left">Wed</td>
<td align="right">-0.22</td>
<td align="right">-0.16</td>
<td align="right">-0.29</td>
</tr>
<tr class="odd">
<td align="left">Thu</td>
<td align="right">-0.33</td>
<td align="right">-0.38</td>
<td align="right">+0.09</td>
</tr>
<tr class="even">
<td align="left">Fri</td>
<td align="right">-0.23</td>
<td align="right">+0.03</td>
<td align="right">+0.20</td>
</tr>
<tr class="odd">
<td align="left">Sat</td>
<td align="right">-0.38</td>
<td align="right">+0.07</td>
<td align="right">-0.11</td>
</tr>
</tbody>
</table>
<p>This table represents the assumed “day-of-the-week” effect to subtract from my raw readings.</p>
<p>With these adjustments, how does my weight projectory now look?</p>
<p><img src="/post/2018-01-06-weighing-up-the-data_files/figure-html/plot.wt.adj-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Not surprisingly, the line of best fit looks similar with a similar spread of points above and below the line.</p>
<p>Now, let’s see how the residuals look. They certainly appear as though they are closer to zero.</p>
<p><img src="/post/2018-01-06-weighing-up-the-data_files/figure-html/daily.resid.adj-1.png" width="960" style="display: block; margin: auto;" /></p>
<p><img src="/post/2018-01-06-weighing-up-the-data_files/figure-html/wt.adj.resid.dow-1.png" width="672" /></p>
<p>One final piece of analysis: how do the</p>
<pre class="r"><code>wts.adj.pred %&gt;%
  group_by(period) %&gt;%
  summarise(resid.mad = mad(resid),
            resid.sd = sd(resid, na.rm = TRUE),
            pred.corr = cor(pred, wt),
            resid.raw.mad = mad(resid.raw),
            resid.raw.sd = sd(resid.raw, na.rm = TRUE),
            pred.raw.corr = cor(pred.raw, wt)) %&gt;%
  knitr::kable(caption = &quot;Residual scores&quot;,
               digits = 3)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-1">Table 1: </span>Residual scores</caption>
<thead>
<tr class="header">
<th align="left">period</th>
<th align="right">resid.mad</th>
<th align="right">resid.sd</th>
<th align="right">pred.corr</th>
<th align="right">resid.raw.mad</th>
<th align="right">resid.raw.sd</th>
<th align="right">pred.raw.corr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Pre-Aug</td>
<td align="right">0.560</td>
<td align="right">0.595</td>
<td align="right">0.961</td>
<td align="right">0.582</td>
<td align="right">0.635</td>
<td align="right">0.955</td>
</tr>
<tr class="even">
<td align="left">Aug-Dec</td>
<td align="right">0.470</td>
<td align="right">0.476</td>
<td align="right">0.617</td>
<td align="right">0.492</td>
<td align="right">0.518</td>
<td align="right">0.517</td>
</tr>
<tr class="odd">
<td align="left">Post Xmas</td>
<td align="right">0.497</td>
<td align="right">0.485</td>
<td align="right">0.512</td>
<td align="right">0.475</td>
<td align="right">0.532</td>
<td align="right">0.354</td>
</tr>
</tbody>
</table>
<p>We can see that there is a marginal improvement in the residuals as a result of this transformation.</p>
</div>
<div id="autoregression-in-the-residuals" class="section level2">
<h2>Autoregression in the Residuals</h2>
<pre class="r"><code>par( mar=c(3.1, 4.7, 2.3, 0),
     cex.lab = 0.8,
     cex.axis = 0.8) 

wts.pred %&gt;%
  filter(date &gt; ymd(&quot;2017/8/1&quot;), date &lt;= ymd(&quot;2017/12/24&quot;)) %&gt;%
  arrange(date) %&gt;%
  select(resid) %&gt;%
  as.ts() %&gt;%
  Pacf(main = &quot;&quot;, #Autocorrelation function of the residuals&quot;,
      sub = &quot;Minor correlation between the residuals from one day to the next, and one day to 10 days time&quot;,
      xlab = &quot;&quot;, #Lag between pairs of days&quot;,
      ylab = &quot;&quot;, #&quot;Correlation factor of residuals between pairs of days&quot;,
      cex.lab = 0.8,
      # mgp=c(-1, 0, 0),
      cex.axis = 0.8)
mtext(side=1, text=&quot;Lag between pairs of days&quot;, line=1.8)
mtext(side=2, text=&quot;Correlation factor of weights between pairs of days&quot;, line=1.8)
mtext(side=3, text=&quot;Partial autocorrelation function of the weights&quot;, line=0.5)</code></pre>
<p><img src="/post/2018-01-06-weighing-up-the-data_files/figure-html/resid.acf-1.png" width="672" /></p>
</div>
</div>
